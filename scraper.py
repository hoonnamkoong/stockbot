import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import time


import sys

# sys.stdout.reconfigure ì œê±° (Next.js í™˜ê²½ë³€ìˆ˜ ì œì–´)

def get_top_trending_stocks(market_type='KOSPI'):

    """
    ë„¤ì´ë²„ ê¸ˆìœµ ê±°ë˜ìƒìœ„(ë˜ëŠ” ì¸ê¸° ê²€ìƒ‰) ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    market_type: 'KOSPI' or 'KOSDAQ'
    """
    sosok = '0' if market_type == 'KOSPI' else '1'
    url = f"https://finance.naver.com/sise/sise_quant.naver?sosok={sosok}" 
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    exclude_keywords = ['KODEX', 'TIGER', 'ETN', 'KBSTAR', 'ACE', 'KOSEF', 'SOL', 'HANARO', 'ARIRANG']
    
    try:
        if market_type == 'KOSPI':
             print(f"[DEBUG] Fetching KOSPI trending stocks...", flush=True)
        else:
             print(f"[DEBUG] Fetching KOSDAQ trending stocks...", flush=True)

        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content.decode('euc-kr', 'replace'), 'html.parser')
        
        table = soup.select_one('table.type_2')
        if table:
            rows = table.select('tr')
            
            data = []
            for row in rows:
                cols = row.select('td')
                if len(cols) < 10: 
                    continue
                
                try:
                    name_tag = cols[1].select_one('a')
                    if not name_tag:
                        continue
                        
                    name = name_tag.get_text(strip=True)
                    
                    # 1. ETF/ETN ì œì™¸ í•„í„°ë§
                    is_excluded = False
                    for kw in exclude_keywords:
                        if kw in name.upper():
                            is_excluded = True
                            break
                    if is_excluded:
                        continue

                    url_suffix = name_tag['href']
                    code = url_suffix.split('code=')[-1]
                    
                    price_str = cols[2].get_text(strip=True).replace(',', '')
                    current_price = int(price_str) if price_str.isdigit() else 0
                    
                    # ë“±ë½ë¥ 
                    change_rate = cols[4].get_text(strip=True).strip()
                    
                    # ì „ì¼ ì¢…ê°€ íŒŒì‹± (ë¦¬ìŠ¤íŠ¸ì— 'ì „ì¼ë¹„' ì»¬ëŸ¼ì´ ìˆìŒ[3], ë“±ë½í­ì„. )
                    # ê±°ë˜ìƒìœ„ ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼: ìˆœìœ„, ì¢…ëª©ëª…, í˜„ì¬ê°€[2], ì „ì¼ë¹„[3], ë“±ë½ë¥ [4], ê±°ë˜ëŸ‰[5], ê±°ë˜ëŒ€ê¸ˆ[6], ë§¤ìˆ˜í˜¸ê°€[7], ë§¤ë„í˜¸ê°€[8], ì‹œê°€ì´ì•¡[9], PER[10], ROE[11] ... 
                    # ì™¸êµ­ì¸ë¹„ìœ¨ì€ ê¸°ë³¸ ì»¬ëŸ¼ì— ì—†ì„ ìˆ˜ ìˆìŒ -> ìƒì„¸ í˜ì´ì§€ íŒŒì‹± í•„ìš”
                    
                    # ì¼ë‹¨ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìµœëŒ€í•œ í™•ë³´
                    change_amount_str = cols[3].get_text(strip=True).replace(',', '')
                    # ìƒìŠ¹/í•˜ë½ ì´ë¯¸ì§€ ë˜ëŠ” í´ë˜ìŠ¤ í™•ì¸ì´ í•„ìš”í•˜ë‚˜, ì¼ë‹¨ ì ˆëŒ€ê°’ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìŒ.
                    # ë“±ë½ë¥  ë¶€í˜¸ë¥¼ ë³´ê³  ì „ì¼ ì¢…ê°€ ì—­ì‚°ì´ ë” ì •í™•í•  ìˆ˜ ìˆìŒ.
                    # ì „ì¼ì¢…ê°€ = í˜„ì¬ê°€ / (1 + ë“±ë½ë¥ /100)
                    
                    prev_close = 0
                    try:
                        rate_float = float(change_rate.replace('%', ''))
                        prev_close = int(current_price / (1 + rate_float/100))
                    except:
                        pass
                    
                    # ì™¸êµ­ì¸ ë¹„ìœ¨ì€ ë³´í†µ ë¦¬ìŠ¤íŠ¸ ë§¨ ë’¤ìª½ì— ìˆì„ ìˆ˜ë„ ìˆìŒ (ì„¤ì • ë”°ë¼ ë‹¤ë¦„)
                    # ì—¬ê¸°ì„œëŠ” ìƒì„¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ ì›ì¹™ìœ¼ë¡œ í•¨ (ì‚¬ìš©ì ìš”ì²­ì‚¬í•­ ì¤€ìˆ˜)

                    stock_info = {
                        'market': market_type,
                        'code': code,
                        'name': name,
                        'price': current_price,
                        'prev_close': prev_close, # ê³„ì‚°ëœ ì „ì¼ ì¢…ê°€ (ì„ì‹œ)
                        'change_rate': change_rate
                    }
                    data.append(stock_info)
                    
                except Exception as e:
                    continue
            
            return data[:100] # ìƒìœ„ 100ê°œë¡œ í™•ëŒ€
        else:
            print(f"Stock table NOT found for {market_type}")
            return []

    except Exception as e:
        print(f"Error fetching trending stocks for {market_type}: {e}")
        return []


def get_stock_details(code):
    """
    íŠ¹ì • ì¢…ëª©ì˜ ìƒì„¸ ì •ë³´(ì „ì¼ì¢…ê°€, ì™¸êµ­ì¸ì†Œì§„ìœ¨ ì´ë ¥ ë“±)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ì¼ë³„ ì‹œì„¸ í˜ì´ì§€(sise_day.naver)ë¥¼ í™œìš©í•©ë‹ˆë‹¤.
    """
    url = f"https://finance.naver.com/item/sise_day.naver?code={code}"
    try:
        response = requests.get(url)
        # response.raise_for_status() # ê°€ë” 403 ëœ° ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜. í—¤ë” ì¶”ê°€ ê¶Œì¥.
        
        # í—¤ë”ê°€ ì—†ìœ¼ë©´ ì°¨ë‹¨ë  ìˆ˜ ìˆìŒ
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì¼ë³„ ì‹œì„¸ í…Œì´ë¸” (type2)
        table = soup.select_one('table.type2')
        if not table:
            return {}
            
        # ë°ì´í„° í–‰(tr) ì¶”ì¶œ (onmouseover ì†ì„± ìˆëŠ” í–‰ë“¤ì´ ë°ì´í„° í–‰ì„)
        rows = table.find_all('tr', {'onmouseover': True})
        
        details = {}
        
        # ì˜¤ëŠ˜(ìµœì‹ ) ë°ì´í„°: rows[0]
        if len(rows) > 0:
            cols_today = rows[0].select('td')
            # ì»¬ëŸ¼ ì¸ë±ìŠ¤(ì¶”ì •): ë‚ ì§œ(0), ì¢…ê°€(1), ì „ì¼ë¹„(2), ì‹œê°€(3), ê³ ê°€(4), ì €ê°€(5), ê±°ë˜ëŸ‰(6)
            # ê·¸ëŸ°ë° ì™¸êµ­ì¸ ë¹„ìœ¨ì€ sise_dayì— ì—†ìŒ. -> ì•„ë¿”ì‹¸. 
            # sise_dayì—ëŠ” ê°€ê²© ì •ë³´ë§Œ ìˆê³  ì™¸êµ­ì¸ ì§€ë¶„ìœ¨ì€ ì—†ìŒ.
            # ì™¸êµ­ì¸ ì§€ë¶„ìœ¨ ì´ë ¥ì€ 'frgn_man.naver' (íˆ¬ììë³„ ë§¤ë§¤ë™í–¥) ì— ìˆìŒ? ì•„ë‹˜ 'sise_day' ë§ê³  ë‹¤ë¥¸ í˜ì´ì§€?
            # ë„¤ì´ë²„ ê¸ˆìœµ -> ì‹œì„¸ -> ì¼ë³„ì‹œì„¸ í˜ì´ì§€ì—ëŠ” ì™¸êµ­ì¸ ì†Œì§„ìœ¨ì´ ì—†ìŒ.
            # "ì¢…í•©ì •ë³´ > íˆ¬ììë³„ ë§¤ë§¤ë™í–¥ > ì™¸êµ­ì¸ ë³´ìœ ìœ¨" íƒ­ì´ ë”°ë¡œ ìˆìŒ. 
            pass

    except Exception as e:
        pass
        
    # ë‹¤ì‹œ ê³„íš ìˆ˜ì •: 
    # 1. í˜„ì¬ ì™¸êµ­ì¸ ë¹„ìœ¨ -> main.naverì—ì„œ ê°€ì ¸ì˜´ (ì´ë¯¸ êµ¬í˜„ë¨)
    # 2. ì–´ì œ ì™¸êµ­ì¸ ë¹„ìœ¨ -> frgn_man.naver (íˆ¬ììë³„ ë§¤ë§¤ë™í–¥) í˜ì´ì§€ íŒŒì‹± í•„ìš”.
    
    # frgn_man.naver URL: https://finance.naver.com/item/frgn.naver?code={code}
    url_frgn = f"https://finance.naver.com/item/frgn.naver?code={code}"
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url_frgn, headers=headers)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # 'ë³´ìœ ìœ¨' í…ìŠ¤íŠ¸ í¬í•¨ëœ í…Œì´ë¸” ì°¾ê¸°
        tables = soup.select('table')
        target_table = None
        
        for t in tables:
            if 'ì™¸êµ­ì¸' in t.get_text() and 'ë³´ìœ ìœ¨' in t.get_text():
                target_table = t
                break
        
        if target_table:
            # í—¤ë” ì œì™¸, ë°ì´í„° í–‰ ì°¾ê¸°
            # êµ¬ì¡°: 
            # Row 0: Header
            # Row 1: Sub-Header
            # Row 2: Spacer (empty)
            # Row 3: Data (Real Today)
            # í•˜ì§€ë§Œ ì¥ì¤‘/ì¥ë§ˆê°ì— ë”°ë¼ í–‰ ê°œìˆ˜ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. 
            # ê°„ë‹¨íˆ trì„ ëª¨ë‘ ê°€ì ¸ì™€ì„œ td ê°œìˆ˜ê°€ ë§ì€ í–‰ì„ ë°ì´í„°ë¡œ ê°„ì£¼
            
            rows = target_table.select('tr')
            data_rows = []
            for row in rows:
                cols = row.select('td')
                if len(cols) > 5: # ë°ì´í„° í–‰ì€ ìµœì†Œ 6ê°œ ì´ìƒ ì»¬ëŸ¼
                    data_rows.append(row)
            
            # ì˜¤ëŠ˜(0), ì–´ì œ(1)
            if len(data_rows) >= 2:
                cols_today = data_rows[0].select('td')
                cols_yest = data_rows[1].select('td')
                
                if len(cols_today) > 0:
                    details['foreign_rate'] = cols_today[-1].get_text(strip=True)
                
                if len(cols_yest) > 0:
                     details['prev_foreign_rate'] = cols_yest[-1].get_text(strip=True)
                     
                     # ì–´ì œ ì¢…ê°€ (ì¸ë±ìŠ¤ 1)
                     prev_close_str = cols_yest[1].get_text(strip=True).replace(',', '')
                     if prev_close_str.isdigit():
                        details['prev_close'] = int(prev_close_str)
        
    except Exception as e:
        print(f"Error fetching foreign details for {code}: {e}")
        
    return details




def get_discussion_stats(code):
    """
    íŠ¹ì • ì¢…ëª© í† ë¡ ì‹¤ì˜ ê²Œì‹œê¸€ ì •ë³´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    - ë‹¹ì¼ 09:00 ì´í›„ ê²Œì‹œê¸€ ì •ë°€ ì¹´ìš´íŒ…
    - ì „ìˆ˜ ì¡°ì‚¬ë¥¼ ìœ„í•´ í˜ì´ì§€ë„¤ì´ì…˜ ìˆ˜í–‰
    """
    
    # ê¸°ì¤€ ì‹œê°„ ì„¤ì • (ë‹¹ì¼ 09:00)
    # ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” 'í˜„ì¬ ë‚ ì§œ' ê¸°ì¤€ 09:00ë¡œ ì„¤ì •
    now = datetime.now()
    target_time = now.replace(hour=9, minute=0, second=0, microsecond=0)
    # ë§Œì•½ í˜„ì¬ ì‹œê°„ì´ 09:00 ì´ì „ì´ë¼ë©´? -> ì „ì¼ 09:00? ì•„ë‹ˆë©´ ë‹¹ì¼ 0ì‹œ?
    # ë³´í†µ ì¥ ì‹œì‘ ì´í›„ë¥¼ ì˜ë¯¸í•˜ë¯€ë¡œ, 9ì‹œ ì´ì „ì´ë©´ "ì•„ì§ ì¥ ì‹œì‘ ì „"ì´ë¼ ê²Œì‹œê¸€ì´ ì ì„ ìˆ˜ ìˆìŒ.
    # ì¼ë‹¨ 'ì˜¤ëŠ˜ 9ì‹œ' ê¸°ì¤€ìœ¼ë¡œ ì¡ë˜, í˜„ì¬ê°€ 9ì‹œ ì´ì „ì´ë©´ 'ì–´ì œ 9ì‹œ'ë¶€í„°?
    # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: "ë‹¹ì¼ 09:00 ì´í›„" -> ëª…í™•í•¨.
    
    if now < target_time:
        # 9ì‹œ ì´ì „ì´ë©´ ì¹´ìš´íŠ¸ 0ì¼ ìˆ˜ ìˆìŒ. (í˜¹ì€ ì–´ì œ ê¸€ì„ ë³´ë¼ëŠ” ê±´ì§€? ì¼ë‹¨ ë¬¸ì ê·¸ëŒ€ë¡œ ë‹¹ì¼ 09ì‹œ ê¸°ì¤€)
        pass # ê·¸ëƒ¥ ì§„í–‰ (09:00 > ê²Œì‹œê¸€ ë‚ ì§œ ì´ë¯€ë¡œ loop ë°”ë¡œ ì¢…ë£Œë  ê²ƒì„)

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    collected_posts = []
    page = 1
    max_pages = 20 # ë¬´í•œ ë£¨í”„ ë°©ì§€ìš© ì•ˆì „ ì¥ì¹˜
    stop_collecting = False
    
    headers['Referer'] = f"https://finance.naver.com/item/board.naver?code={code}"

    while page <= max_pages and not stop_collecting:
        url = f"https://finance.naver.com/item/board.naver?code={code}&page={page}"
        
        try:
            # ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ ë°©ì§€
            if page > 1:
                time.sleep(0.5)

            # íƒ€ì„ì•„ì›ƒ 10ì´ˆ ì„¤ì •
            response = requests.get(url, headers=headers, timeout=10)
            # BS4 ìë™ ê°ì§€ ë§¡ê¹€

            soup = BeautifulSoup(response.content, 'html.parser')
            
            table = soup.select_one('table.type2')
            if not table:
                break
                
            rows = table.select('tr')
            # ê²Œì‹œê¸€ ì—†ìœ¼ë©´ ì¢…ë£Œ
            if not rows: 
                break
                
            found_post_in_page = False
            
            for row in rows:
                cols = row.select('td')
                if len(cols) < 5:
                    continue
                
                try:
                    # ë‚ ì§œ í™•ì¸
                    # ë„¤ì´ë²„ ê¸ˆìœµ ë‚ ì§œ í¬ë§·: "2024.05.21 14:30"
                    date_text = cols[0].get_text(strip=True)
                    
                    # ë‚ ì§œ í˜•ì‹ì´ ë§ëŠ”ì§€ í™•ì¸ (ê°€ë” ê³µì§€ì‚¬í•­ ë“±ì´ ì„ì¼ ìˆ˜ ìˆìŒ)
                    try:
                        post_date = datetime.strptime(date_text, "%Y.%m.%d %H:%M")
                    except ValueError:
                        # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ (í—¤ë”ë‚˜ ê³µì§€ì¼ ìˆ˜ ìˆìŒ)
                        continue
                        
                    found_post_in_page = True

                    # ê¸°ì¤€ ì‹œê°„ ì²´í¬
                    if post_date < target_time:
                        stop_collecting = True
                        break # ë” ì´ìƒ ë³¼ í•„ìš” ì—†ìŒ (ê³¼ê±° ê¸€)
                    
                    # ìˆ˜ì§‘ ëŒ€ìƒ
                    title = ""
                    title_tag = row.select_one('a.title')
                    if not title_tag and len(cols) > 1:
                        title_tag = cols[1].select_one('a')
                    
                    if title_tag:
                         title = title_tag.get_text(strip=True)
                    
                    views = cols[3].get_text(strip=True)

                    collected_posts.append({
                        'title': title,
                        'date': date_text, # ì›ë³¸ í…ìŠ¤íŠ¸ ìœ ì§€ (í‘œì‹œìš©)
                        'views': views
                    })
                    
                except Exception:
                    continue
            
            # í˜ì´ì§€ì— ìœ íš¨í•œ ê²Œì‹œê¸€ íƒœê·¸ê°€ í•˜ë‚˜ë„ ì—†ì—ˆë‹¤ë©´? (êµ¬ì¡° ë³€ê²½ ë“±) -> ë‹¤ìŒ í˜ì´ì§€ ê°€ë´ì•¼ í•¨?
            # ì•„ë‹ˆë©´ ê·¸ëƒ¥ ì¢…ë£Œ?
            # ì¼ë‹¨ found_post_in_pageê°€ Falseì—¬ë„(ê³µì§€ì‚¬í•­ë§Œ ìˆê±°ë‚˜) ë‹¤ìŒ í˜ì´ì§€ ì‹œë„í•  ìˆ˜ ìˆìŒ.
            # í•˜ì§€ë§Œ ë³´í†µ 1í˜ì´ì§€ì— ì—†ìœ¼ë©´ ë°ì´í„°ê°€ ì—†ëŠ” ê²ƒ.
            
            page += 1
            
        except Exception as e:
            print(f"Error fetching page {page} for {code}: {e}")
            break
            
    return {
        'code': code,
        'recent_posts_count': len(collected_posts), # ì •ë°€ ì¹´ìš´íŒ… ëœ ìˆ«ì
        'latest_posts': collected_posts[:5], # ë¶„ì„ìš©ìœ¼ë¡œëŠ” ì „ì²´ê°€ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜, ë¦¬í„´ì€ ì¼ë¶€ë§Œ (Analyzerì—ëŠ” ì „ì²´ ì „ë‹¬ í•„ìš”í•˜ë©´ êµ¬ì¡° ìˆ˜ì •)
        'all_posts_titles': [p['title'] for p in collected_posts] # ê°ì„± ë¶„ì„ìš© ì „ì²´ ì œëª© ë¦¬ìŠ¤íŠ¸
    }




import analyzer
from src import research_scraper
from src import utils # For robust telegram sending if needed, or use telegram_plugin

def load_env_manual(filepath=".env.local"):
    # ... (existing code) ...
    pass

if __name__ == "__main__":
    # 0. Load Environment Variables
    load_env_manual()
    
    # ... check time ...
    
    # 1. Research Briefing (Enabled)
    print("\n[Research] Updating Market Briefing & PDF Analysis...")
    try:
        # Check if research_scraper has main() or fetch_research_data()
        # Based on previous view, it has main() which saves json.
        # We should call main() or the core function.
        # research_scraper.main() seems to do everything including saving JSON.
        research_scraper.main()
        print("[Research] Completed.")
        
        # Send Research Telegram
        try:
            import json
            with open('data/latest_research.json', 'r', encoding='utf-8') as f:
                r_data = json.load(f)
            
            invest_summary = r_data.get('invest', {}).get('summary', 'ìš”ì•½ ì—†ìŒ')
            items_count = r_data.get('invest', {}).get('today_count', 0)
            
            r_msg = f"ğŸ“‘ [ë¦¬í¬íŠ¸ ë¸Œë¦¬í•‘] ì´ {items_count}ê±´\n\n"
            r_msg += f"ğŸ’¡ ì‹œì¥ ìš”ì•½: {invest_summary[:300]}...\n\n"
            r_msg += f"ğŸ‘‰ ìì„¸íˆ ë³´ê¸°: {os.environ.get('DASHBOARD_URL', '')}"
            
            import telegram_plugin
            telegram_plugin.send_telegram_message(r_msg)
            print("[Research] Telegram Sent.")
            
        except Exception as tg_e:
            print(f"[Research] Telegram Error: {tg_e}")
            
    except Exception as e:
        print(f"[Research] Error: {e}")

    markets = ['KOSPI', 'KOSDAQ']
    # ... (rest of code) ...
    
    all_data = [] # í†µí•© ë°ì´í„° ì €ì¥ìš©

    for market in markets:
        print(f"\n[{market}] Starting collection...")
        # Get MORE stocks to ensure we find enough active ones (Top 100 instead of 20)
        trending_stocks = get_top_trending_stocks(market)
        # Limit to top 100 for performance (get_top_trending_stocks needs update to return more)
        # Assuming get_top_trending_stocks returns whatever it finds on page (usually 100 if not sliced)
        
        # In this edited version, we'll slice larger
        source_count = len(trending_stocks)
        print(f"Found {source_count} stocks in {market} Top list.")
        
        count_collected = 0
        
        for i, stock in enumerate(trending_stocks):
            # Performance safety / Limit (User Request V6.2: 30 stocks)
            if i >= 30: break 
            
            # 1. ìƒì„¸ ì •ë³´ (ì „ì¼ì¢…ê°€, ì™¸êµ­ì¸)
            details = get_stock_details(stock['code'])
            stock.update(details)
            
            # 2. í† ë¡ ë°© ì •ë³´ (ì‹œê°„ ê¸°ì¤€ ì¹´ìš´íŒ…)
            stats = get_discussion_stats(stock['code'])
            recent_count = stats.get('recent_posts_count', 0)
            
            # FILTER HERE
            if recent_count >= threshold:
                stock['recent_posts_count'] = recent_count
                stock['latest_posts'] = stats.get('latest_posts', [])
                stock['all_posts_titles'] = stats.get('all_posts_titles', []) 
                
                all_data.append(stock)
                count_collected += 1
                print(f" [KEEP] {stock['name']}: {recent_count} posts (Threshold {threshold})")
            else:
                # print(f" [SKIP] {stock['name']}: {recent_count} posts")
                pass

        print(f"Collected {count_collected} items from {market} meeting criteria.")

    if all_data:
        print(f"\nAnalyzing total {len(all_data)} items...")
        result_df = analyzer.analyze_discussion_trend(all_data)
        
        # í†µí•© íŒŒì¼ë¡œ ì €ì¥
        filename = f"trending_integrated"
        analyzer.save_to_csv(result_df, filename_prefix=filename)

        # Telegram Notification
        try:
            import telegram_plugin
            import os
            
            # Format message
            # Split Messages (User Request V6.6: Link FIRST, then Data)
            
            # 1. Dashboard Link (FIRST PRIORITY)
            dashboard_url = os.environ.get('DASHBOARD_URL', '')
            if dashboard_url:
                 telegram_plugin.send_telegram_message(f"ğŸ“Š <b>Dashboard Check</b>\n{dashboard_url}")
                 time.sleep(1)

            # 2. KOSPI Message
            kospi_stocks = [x for x in all_data if x['market']=='KOSPI']
            if kospi_stocks:
                sorted_k = sorted(kospi_stocks, key=lambda x: x['recent_posts_count'], reverse=True)
                msg_k = f"ğŸ“‰ [KOSPI] ({len(kospi_stocks)} items)\n"
                for s in sorted_k[:10]: # Top 10 only per message
                     msg_k += f"ğŸ”¥ <b>{s['name']}</b>: {s['recent_posts_count']}ê¸€ | {s.get('change_rate','-')}\n"
                
                # Check message length safety (optional, but good practice)
                telegram_plugin.send_telegram_message(msg_k)
                time.sleep(1)

            # 3. KOSDAQ Message
            kosdaq_stocks = [x for x in all_data if x['market']=='KOSDAQ']
            if kosdaq_stocks:
                sorted_q = sorted(kosdaq_stocks, key=lambda x: x['recent_posts_count'], reverse=True)
                msg_q = f"ğŸ“‰ [KOSDAQ] ({len(kosdaq_stocks)} items)\n"
                for s in sorted_q[:10]:
                     msg_q += f"ğŸ”¥ <b>{s['name']}</b>: {s['recent_posts_count']}ê¸€ | {s.get('change_rate','-')}\n"
                telegram_plugin.send_telegram_message(msg_q)
            
        except ImportError:
            pass
        except Exception as e:
            print(f"Failed to send finish notification: {e}")

    else:
        print("No data collected meeting the threshold.")
        # User Request: Send notification even if empty, so we know it ran.
        try:
            import telegram_plugin
            import os
            
            dashboard_url = os.environ.get('DASHBOARD_URL', '')
            
            # 1. Dashboard Link (Checking Alive)
            if dashboard_url:
                 telegram_plugin.send_telegram_message(f"ğŸ“Š <b>Dashboard Check (No Data)</b>\n{dashboard_url}")
                 time.sleep(1)
                 
            # 2. Status Message
            msg = f"ğŸ“‰ [Report] {datetime.now().strftime('%H:%M')}\n"
            msg += f"Threshold: {threshold} posts\n"
            msg += "Info: ì¡°ê±´ì— ë§ëŠ” ê¸‰ìƒìŠ¹ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤. (No stocks found)"
            
            telegram_plugin.send_telegram_message(msg)
            
        except Exception as e:
            print(f"Failed to send empty notification: {e}")







