import requests
import io
import re
from pypdf import PdfReader

# User-Agent for download
HEADER = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}

# Beginner Glossary
GLOSSARY = {
    'PER': 'ì£¼ê°€ìˆ˜ìµë¹„ìœ¨(PER)ì€ í˜„ì¬ ì£¼ê°€ê°€ 1ì£¼ë‹¹ ìˆœì´ìµì˜ ëª‡ ë°°ì¸ê°€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì €í‰ê°€ë˜ì—ˆë‹¤ê³  ë´…ë‹ˆë‹¤.',
    'PBR': 'ì£¼ê°€ìˆœìì‚°ë¹„ìœ¨(PBR)ì€ ì£¼ê°€ê°€ ìˆœìì‚°(ìë³¸)ì— ë¹„í•´ ëª‡ ë°°ë¡œ ê±°ë˜ë˜ê³  ìˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.',
    'ROE': 'ìê¸°ìë³¸ì´ìµë¥ (ROE)ì€ ê¸°ì—…ì´ ìê¸°ìë³¸ì„ í™œìš©í•´ ì–¼ë§ˆë§Œí¼ì˜ ì´ìµì„ ëƒˆëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ìˆ˜ìµì„± ì§€í‘œì…ë‹ˆë‹¤.',
    'TP': 'TP(Target Price)ëŠ” ì¦ê¶Œì‚¬ê°€ ì˜ˆìƒí•˜ëŠ” í•´ë‹¹ ì£¼ì‹ì˜ ëª©í‘œ ì£¼ê°€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.',
    'Yoy': 'YoY(Year over Year)ëŠ” ì „ë…„ ë™ê¸° ëŒ€ë¹„ ì¦ê°ìœ¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.',
    'Qoq': 'QoQ(Quarter over Quarter)ëŠ” ì§ì „ ë¶„ê¸° ëŒ€ë¹„ ì¦ê°ìœ¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.'
}

def clean_pdf_text(text):
    """ Cleans extracted text, removing headers/footers/disclaimers """
    # Remove single characters standing alone (artifacts)
    text = re.sub(r'\s+.\s+', ' ', text)
    # Remove disclaimers
    if "Compliance" in text:
        text = text.split("Compliance")[0]
    return text.strip()

def download_pdf(url):
    try:
        res = requests.get(url, headers=HEADER)
        if res.status_code == 200:
            return io.BytesIO(res.content)
    except Exception as e:
        print(f"PDF Download Error: {e}")
    return None

def analyze_pdf(pdf_url):
    stream = download_pdf(pdf_url)
    if not stream: return None
    
    try:
        reader = PdfReader(stream)
        # Extract text from first 2 pages (usually sufficient for summary)
        full_text = ""
        for i in range(min(2, len(reader.pages))):
            full_text += reader.pages[i].extract_text() + "\n"
            
        if not full_text.strip():
            return {
                "opinion": "N/A",
                "target_price": "N/A",
                "summary": "í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶ˆê°€ (ì´ë¯¸ì§€ ìŠ¤ìº”ë³¸ì¼ ìˆ˜ ìˆìŒ). OCR ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            }

        # Parsing Logic
        cleaned_text = clean_pdf_text(full_text)
        
        # 1. Opinion
        opinion = "N/A"
        match = re.search(r'(BUY|SELL|HOLD|Reduce|ë§¤ìˆ˜|ì¤‘ë¦½|ë§¤ë„)', cleaned_text, re.IGNORECASE)
        if match:
            opinion = match.group(1).upper()
            
        # 2. Target Price
        tp = "N/A"
        match_tp = re.search(r'(ëª©í‘œì£¼ê°€|Target Price|TP)\D{1,10}([\d,]+)', cleaned_text, re.IGNORECASE)
        if match_tp:
            tp = match_tp.group(2) + "ì›"

        # 3. Structure Extraction (Arguments)
        summary_points = []
        
        # Look for headers
        headers = ['íˆ¬ìí¬ì¸íŠ¸', 'Investment Point', 'ì²´í¬í¬ì¸íŠ¸', 'Key Charts', 'Valuation', 'ê²°ë¡ ']
        sentences = cleaned_text.split('\n')
        
        capture_mode = False
        captured_lines = []
        
        for line in sentences:
            line = line.strip()
            if not line: continue
            
            # Start capturing if header found
            for h in headers:
                if h in line:
                    capture_mode = True
                    summary_points.append(f"\n[{h}]") # Add header as section
                    break
            
            if capture_mode:
                if len(captured_lines) < 10: # Limit to 10 lines of key arguments
                    captured_lines.append(line)
                    summary_points.append(f"- {line}")
            else:
                # If no header found yet, maybe check for numbered lists (1. 2. )
                if re.match(r'^[1-9]\.', line):
                    summary_points.append(f"- {line}")
        
        final_summary = "\n".join(summary_points)
        if not final_summary:
            # Fallback to first 500 chars if no structure found
            final_summary = cleaned_text[:500] + "..."

        # 4. Inject Glossary
        used_glossary = []
        for term, desc in GLOSSARY.items():
            if term in final_summary:
                used_glossary.append(f"ğŸ’¡ {term}: {desc}")
        
        if used_glossary:
            final_summary += "\n\n[ìš©ì–´ ì„¤ëª…]\n" + "\n".join(used_glossary)

        return {
            "opinion": opinion,
            "target_price": tp,
            "summary": final_summary,
            "raw_text_snippet": cleaned_text[:200]
        }

    except Exception as e:
        print(f"PDF Parsing Error: {e}")
        return None
