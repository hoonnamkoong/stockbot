import requests
import io
import re
from pypdf import PdfReader

# User-Agent for download
HEADER = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}

# Beginner Glossary
GLOSSARY = {
    'PER': 'ì£¼ê°€ìˆ˜ìµë¹„ìœ¨(PER) - ë‚®ì„ìˆ˜ë¡ ì €í‰ê°€ (ì´ìµ ëŒ€ë¹„ ì£¼ê°€)',
    'PBR': 'ì£¼ê°€ìˆœìì‚°ë¹„ìœ¨(PBR) - 1 ë¯¸ë§Œì´ë©´ ìì‚°ê°€ì¹˜ë³´ë‹¤ ì €í‰ê°€',
    'ROE': 'ìê¸°ìë³¸ì´ìµë¥ (ROE) - ë†’ì„ìˆ˜ë¡ íš¨ìœ¨ì ì¸ ê²½ì˜ (ë‚´ ëˆìœ¼ë¡œ ë²ˆ ëˆ)',
    'TP': 'TP(Target Price) - ì¦ê¶Œì‚¬ê°€ ì œì‹œí•œ ëª©í‘œ ì£¼ê°€',
    'YoY': 'ì „ë…„ ë™ê¸° ëŒ€ë¹„ ì¦ê°ìœ¨',
    'QoQ': 'ì§ì „ ë¶„ê¸° ëŒ€ë¹„ ì¦ê°ìœ¨',
    'OPM': 'ì˜ì—…ì´ìµë¥  (ë§¤ì¶œ ëŒ€ë¹„ ì˜ì—…ì´ìµ ë¹„ì¤‘)'
}

def clean_pdf_text(text):
    """ 
    Aggressive Cleaning for 'Insight Only' view.
    Removes: Dates, Emails, Phones, URLs, Legal Disclaimers, Headers/Footers.
    """
    if not text: return ""

    # 1. Boilerplate Removal (Compliance, Disclaimer)
    # Truncate text after common disclaimer headers
    disclaimers = ["Compliance Notice", "Compliance", "ê³ ê° ì—¬ëŸ¬ë¶„ê»˜", "íˆ¬ì íŒë‹¨ì˜ ìµœì¢… ì±…ì„", "ë³¸ ì¡°ì‚¬ë¶„ì„ìë£Œ", "Disclosures"]
    for d in disclaimers:
        if d in text:
            text = text.split(d)[0] # Cut off everything after disclaimer start

    # 2. Regex Cleaning
    # Remove Emails
    text = re.sub(r'[\w\.-]+@[\w\.-]+\.\w+', '', text)
    # Remove Phone Numbers
    text = re.sub(r'\d{2,3}[-)\.]\d{3,4}[-)\.]\d{4}', '', text)
    # Remove Dates (YYYY.MM.DD or YYYY-MM-DD) - debatable, but user asked to remove "Article Date"
    text = re.sub(r'\d{4}[\.-]\d{2}[\.-]\d{2}', '', text)
    # Remove Korean Dates (e.g., 2025ë…„ 12ì›” 12ì¼)
    text = re.sub(r'\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼', '', text)
    
    # Remove URLS
    text = re.sub(r'http[s]?://\S+', '', text)
    
    # Remove Chart Axis Garbage (Sequence of numbers like "0 10 20 30 40")
    # Pattern: 2+ digits, space, 2+ digits, space, 2+ digits... repeated
    text = re.sub(r'(\b\d{1,4}\s+){3,}\d{1,4}', '', text)

    # 3. Artifact/Spacing Cleaning
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def download_pdf(url):
    try:
        res = requests.get(url, headers=HEADER, timeout=10)
        if res.status_code == 200:
            return io.BytesIO(res.content)
    except Exception as e:
        print(f"PDF Download Error: {e}")
    return None

def analyze_pdf(pdf_url, web_body_text=""):
    """
    Analyzes PDF and optionally merges insights with Web Body Text.
    """
    stream = download_pdf(pdf_url)
    if not stream: return None
    
    try:
        reader = PdfReader(stream)
        # Extract text from first 2 pages
        full_text = ""
        for i in range(min(2, len(reader.pages))):
            full_text += reader.pages[i].extract_text() + "\n"
            
        if not full_text.strip():
            return {
                "opinion": "N/A",
                "target_price": "N/A",
                "summary": "í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶ˆê°€ (ì´ë¯¸ì§€ ìŠ¤ìº”ë³¸ì¼ ìˆ˜ ìˆìŒ). ìš°ì¸¡ ì›¹ ìš”ì•½ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”."
            }

        # Parsing Logic
        cleaned_text = clean_pdf_text(full_text)
        
        # 1. Opinion & TP
        opinion = "N/A"
        match = re.search(r'(BUY|SELL|HOLD|Reduce|ë§¤ìˆ˜|ì¤‘ë¦½|ë§¤ë„)', cleaned_text, re.IGNORECASE)
        if match: opinion = match.group(1).upper()
            
        tp = "N/A"
        match_tp = re.search(r'(ëª©í‘œì£¼ê°€|Target Price|TP)\D{0,10}([\d,]+)', cleaned_text, re.IGNORECASE)
        if match_tp: tp = match_tp.group(2) + "ì›"

        # 2. Structure Extraction
        summary_points = []
        
        # Priority Headers (Mapped to standard names)
        header_map = {
            'íˆ¬ìí¬ì¸íŠ¸': 'ğŸ’¡ í•µì‹¬ íˆ¬ì í¬ì¸íŠ¸',
            'Investment Point': 'ğŸ’¡ í•µì‹¬ íˆ¬ì í¬ì¸íŠ¸',
            'ì²´í¬í¬ì¸íŠ¸': 'ğŸ’¡ í•µì‹¬ íˆ¬ì í¬ì¸íŠ¸',
            'ê²°ë¡ ': 'ğŸ“Œ ê²°ë¡ ',
            'Conclusion': 'ğŸ“Œ ê²°ë¡ ',
            'Valuation': 'ğŸ“Š ë°¸ë¥˜ì—ì´ì…˜',
            'ë¦¬ìŠ¤í¬': 'âš ï¸ ë¦¬ìŠ¤í¬ ìš”ì¸'
        }
        
        sentences = cleaned_text.split('. ')
        
        current_section = None
        captured_content = []
        
        # Simple extraction strategy: If a sentence contains a header keyword, start a section.
        for sent in sentences:
            sent = sent.strip()
            if len(sent) < 10: continue
            
            # Check for header
            found_header = False
            for key, label in header_map.items():
                if key in sent:
                    current_section = label
                    summary_points.append(f"\n{current_section}")
                    found_header = True
                    break
            
            if not found_header and current_section:
                # Add to current section
                summary_points.append(f"- {sent}.")
                
            if len(summary_points) > 20: break # Cap length
            
        final_summary = "\n".join(summary_points)
        
        # 3. Tables & Financial Data Extraction (Pseudo-OCR)
        # Attempt to find financial summary lines (Year, P/E, ROE, Revenue)
        # Pattern: Term followed by multiple numbers
        financial_keywords = ['ë§¤ì¶œì•¡', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ', 'PER', 'PBR', 'ROE', 'EPS', 'Revenue', 'Operating Profit', 'Net Income']
        found_data_rows = []
        
        lines = cleaned_text.split('\n') # Use original lines if possible, but cleaned_text is joined.
        # Retry splitting cleaned text by spacing is hard. Let's look for regex patterns in the big text block.
        # Actually, split text on multiple spaces might work 
        
        # Simple Approach: Look for patterns in the raw full_text (before aggressive join) to find table rows
        # But we only have cleaned_text here efficiently. Let's scan for keyword + numbers.
        
        table_md = ""
        extracted_rows = []
        for kw in financial_keywords:
            # Regex: Keyword ... number ... number ... number
            # e.g. "PER 10.5 12.3 9.8"
            match = re.search(f"({kw})\s+([\d\.,\s]+)", cleaned_text)
            if match:
                val_str = match.group(2).strip()
                # Check if it looks like a sequence of data
                if len(val_str.split()) >= 2: 
                     extracted_rows.append(f"| {kw} | {val_str} |")

        if extracted_rows:
            table_md = "\n\n### ğŸ“Š ì£¼ìš” ì¬ë¬´ ë°ì´í„° (ì¶”ì •)\n| í•­ëª© | ë°ì´í„° (ì—°ë„ë³„ ì¶”ì´) |\n|---|---|\n" + "\n".join(extracted_rows)
            final_summary += table_md

        # Fallback: If no structure found, use Web Body or raw text
        if not final_summary.strip():
            if web_body_text:
                final_summary = f"[ì›¹ ë³¸ë¬¸ ê¸°ë°˜ ìš”ì•½]\n{web_body_text[:500]}..."
            else:
                final_summary = cleaned_text[:500] + "..."

        # 4. Inject Glossary
        used_glossary = []
        for term, desc in GLOSSARY.items():
            if term in final_summary or term in cleaned_text:
                used_glossary.append(f"â“ {term}: {desc}")
        
        if used_glossary:
            final_summary += "\n\n" + "\n".join(used_glossary)

        return {
            "opinion": opinion,
            "target_price": tp,
            "summary": final_summary,
            "raw_text_snippet": cleaned_text[:300] + "..."
        }

    except Exception as e:
        print(f"PDF Parsing Error: {e}")
        return None
