import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import time
import os
import sys



def get_top_trending_stocks(market_type='KOSPI'):

    """
    ë„¤ì´ë²„ ê¸ˆìœµ ê±°ë˜ìƒìœ„(ë˜ëŠ” ì¸ê¸° ê²€ìƒ‰) ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    market_type: 'KOSPI' or 'KOSDAQ'
    """
    sosok = '0' if market_type == 'KOSPI' else '1'
    url = f"https://finance.naver.com/sise/sise_quant.naver?sosok={sosok}" 
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://finance.naver.com/',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
    }

    
    exclude_keywords = ['KODEX', 'TIGER', 'ETN', 'KBSTAR', 'ACE', 'KOSEF', 'SOL', 'HANARO', 'ARIRANG']
    
    try:
        if market_type == 'KOSPI':
             print(f"[DEBUG] Fetching KOSPI trending stocks...", flush=True)
        else:
             print(f"[DEBUG] Fetching KOSDAQ trending stocks...", flush=True)

        print(f"[DEBUG] Sending request to {url}...", flush=True)
        response = requests.get(url, headers=headers, timeout=10)
        print(f"[DEBUG] Response Received. Status: {response.status_code}", flush=True)
        response.raise_for_status()

        soup = BeautifulSoup(response.content.decode('euc-kr', 'replace'), 'html.parser')
        
        table = soup.select_one('table.type_2')
        if table:
            rows = table.select('tr')
            
            data = []
            for row in rows:
                cols = row.select('td')
                if len(cols) < 10: 
                    continue
                
                try:
                    name_tag = cols[1].select_one('a')
                    if not name_tag:
                        continue
                        
                    name = name_tag.get_text(strip=True)
                    
                    # 1. ETF/ETN ì œì™¸ í•„í„°ë§
                    is_excluded = False
                    for kw in exclude_keywords:
                        if kw in name.upper():
                            is_excluded = True
                            break
                    if is_excluded:
                        continue

                    url_suffix = name_tag['href']
                    code = url_suffix.split('code=')[-1]
                    
                    price_str = cols[2].get_text(strip=True).replace(',', '')
                    current_price = int(price_str) if price_str.isdigit() else 0
                    
                    # ë“±ë½ë¥ 
                    change_rate = cols[4].get_text(strip=True).strip()
                    
                    # ì „ì¼ ì¢…ê°€ íŒŒì‹± (ë¦¬ìŠ¤íŠ¸ì— 'ì „ì¼ë¹„' ì»¬ëŸ¼ì´ ìˆìŒ[3], ë“±ë½í­ì„. )
                    # ê±°ë˜ìƒìœ„ ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼: ìˆœìœ„, ì¢…ëª©ëª…, í˜„ì¬ê°€[2], ì „ì¼ë¹„[3], ë“±ë½ë¥ [4], ê±°ë˜ëŸ‰[5], ê±°ë˜ëŒ€ê¸ˆ[6], ë§¤ìˆ˜í˜¸ê°€[7], ë§¤ë„í˜¸ê°€[8], ì‹œê°€ì´ì•¡[9], PER[10], ROE[11] ... 
                    # ì™¸êµ­ì¸ë¹„ìœ¨ì€ ê¸°ë³¸ ì»¬ëŸ¼ì— ì—†ì„ ìˆ˜ ìˆìŒ -> ìƒì„¸ í˜ì´ì§€ íŒŒì‹± í•„ìš”
                    
                    # ì¼ë‹¨ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìµœëŒ€í•œ í™•ë³´
                    change_amount_str = cols[3].get_text(strip=True).replace(',', '')
                    # ìƒìŠ¹/í•˜ë½ ì´ë¯¸ì§€ ë˜ëŠ” í´ë˜ìŠ¤ í™•ì¸ì´ í•„ìš”í•˜ë‚˜, ì¼ë‹¨ ì ˆëŒ€ê°’ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìŒ.
                    # ë“±ë½ë¥  ë¶€í˜¸ë¥¼ ë³´ê³  ì „ì¼ ì¢…ê°€ ì—­ì‚°ì´ ë” ì •í™•í•  ìˆ˜ ìˆìŒ.
                    # ì „ì¼ì¢…ê°€ = í˜„ì¬ê°€ / (1 + ë“±ë½ë¥ /100)
                    
                    prev_close = 0
                    try:
                        rate_float = float(change_rate.replace('%', ''))
                        prev_close = int(current_price / (1 + rate_float/100))
                    except:
                        pass
                    
                    # ì™¸êµ­ì¸ ë¹„ìœ¨ì€ ë³´í†µ ë¦¬ìŠ¤íŠ¸ ë§¨ ë’¤ìª½ì— ìˆì„ ìˆ˜ë„ ìˆìŒ (ì„¤ì • ë”°ë¼ ë‹¤ë¦„)
                    # ì—¬ê¸°ì„œëŠ” ìƒì„¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ ì›ì¹™ìœ¼ë¡œ í•¨ (ì‚¬ìš©ì ìš”ì²­ì‚¬í•­ ì¤€ìˆ˜)

                    stock_info = {
                        'market': market_type,
                        'code': code,
                        'name': name,
                        'price': current_price,
                        'prev_close': prev_close, # ê³„ì‚°ëœ ì „ì¼ ì¢…ê°€ (ì„ì‹œ)
                        'change_rate': change_rate
                    }
                    data.append(stock_info)
                    
                except Exception as e:
                    continue
            
            return data[:30] # ìƒìœ„ 30ê°œë¡œ ì¶•ì†Œ (Top 30 Focus - User Request V7.5)
        else:
            print(f"Stock table NOT found for {market_type}")
            return []

    except Exception as e:
        print(f"Error fetching trending stocks for {market_type}: {e}")
        return []


def get_stock_details(code):
    """
    íŠ¹ì • ì¢…ëª©ì˜ ìƒì„¸ ì •ë³´(ì „ì¼ì¢…ê°€, ì™¸êµ­ì¸ì†Œì§„ìœ¨ ì´ë ¥ ë“±)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ì¼ë³„ ì‹œì„¸ í˜ì´ì§€(sise_day.naver)ë¥¼ í™œìš©í•©ë‹ˆë‹¤.
    """
    url = f"https://finance.naver.com/item/sise_day.naver?code={code}"
    try:
        response = requests.get(url)
        # response.raise_for_status() # ê°€ë” 403 ëœ° ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜. í—¤ë” ì¶”ê°€ ê¶Œì¥.
        
        # í—¤ë”ê°€ ì—†ìœ¼ë©´ ì°¨ë‹¨ë  ìˆ˜ ìˆìŒ
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì¼ë³„ ì‹œì„¸ í…Œì´ë¸” (type2)
        table = soup.select_one('table.type2')
        if not table:
            return {}
            
        # ë°ì´í„° í–‰(tr) ì¶”ì¶œ (onmouseover ì†ì„± ìˆëŠ” í–‰ë“¤ì´ ë°ì´í„° í–‰ì„)
        rows = table.find_all('tr', {'onmouseover': True})
        
        details = {}
        
        # ì˜¤ëŠ˜(ìµœì‹ ) ë°ì´í„°: rows[0]
        if len(rows) > 0:
            cols_today = rows[0].select('td')
            # ì»¬ëŸ¼ ì¸ë±ìŠ¤(ì¶”ì •): ë‚ ì§œ(0), ì¢…ê°€(1), ì „ì¼ë¹„(2), ì‹œê°€(3), ê³ ê°€(4), ì €ê°€(5), ê±°ë˜ëŸ‰(6)
            # ê·¸ëŸ°ë° ì™¸êµ­ì¸ ë¹„ìœ¨ì€ sise_dayì— ì—†ìŒ. -> ì•„ë¿”ì‹¸. 
            # sise_dayì—ëŠ” ê°€ê²© ì •ë³´ë§Œ ìˆê³  ì™¸êµ­ì¸ ì§€ë¶„ìœ¨ì€ ì—†ìŒ.
            # ì™¸êµ­ì¸ ì§€ë¶„ìœ¨ ì´ë ¥ì€ 'frgn_man.naver' (íˆ¬ììë³„ ë§¤ë§¤ë™í–¥) ì— ìˆìŒ? ì•„ë‹˜ 'sise_day' ë§ê³  ë‹¤ë¥¸ í˜ì´ì§€?
            # ë„¤ì´ë²„ ê¸ˆìœµ -> ì‹œì„¸ -> ì¼ë³„ì‹œì„¸ í˜ì´ì§€ì—ëŠ” ì™¸êµ­ì¸ ì†Œì§„ìœ¨ì´ ì—†ìŒ.
            # "ì¢…í•©ì •ë³´ > íˆ¬ììë³„ ë§¤ë§¤ë™í–¥ > ì™¸êµ­ì¸ ë³´ìœ ìœ¨" íƒ­ì´ ë”°ë¡œ ìˆìŒ. 
            pass

    except Exception as e:
        pass
        
    # ë‹¤ì‹œ ê³„íš ìˆ˜ì •: 
    # 1. í˜„ì¬ ì™¸êµ­ì¸ ë¹„ìœ¨ -> main.naverì—ì„œ ê°€ì ¸ì˜´ (ì´ë¯¸ êµ¬í˜„ë¨)
    # 2. ì–´ì œ ì™¸êµ­ì¸ ë¹„ìœ¨ -> frgn_man.naver (íˆ¬ììë³„ ë§¤ë§¤ë™í–¥) í˜ì´ì§€ íŒŒì‹± í•„ìš”.
    
    # frgn_man.naver URL: https://finance.naver.com/item/frgn.naver?code={code}
    url_frgn = f"https://finance.naver.com/item/frgn.naver?code={code}"
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url_frgn, headers=headers)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # 'ë³´ìœ ìœ¨' í…ìŠ¤íŠ¸ í¬í•¨ëœ í…Œì´ë¸” ì°¾ê¸°
        tables = soup.select('table')
        target_table = None
        
        for t in tables:
            if 'ì™¸êµ­ì¸' in t.get_text() and 'ë³´ìœ ìœ¨' in t.get_text():
                target_table = t
                break
        
        if target_table:
            # í—¤ë” ì œì™¸, ë°ì´í„° í–‰ ì°¾ê¸°
            # êµ¬ì¡°: 
            # Row 0: Header
            # Row 1: Sub-Header
            # Row 2: Spacer (empty)
            # Row 3: Data (Real Today)
            # í•˜ì§€ë§Œ ì¥ì¤‘/ì¥ë§ˆê°ì— ë”°ë¼ í–‰ ê°œìˆ˜ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. 
            # ê°„ë‹¨íˆ trì„ ëª¨ë‘ ê°€ì ¸ì™€ì„œ td ê°œìˆ˜ê°€ ë§ì€ í–‰ì„ ë°ì´í„°ë¡œ ê°„ì£¼
            
            rows = target_table.select('tr')
            data_rows = []
            for row in rows:
                cols = row.select('td')
                if len(cols) > 5: # ë°ì´í„° í–‰ì€ ìµœì†Œ 6ê°œ ì´ìƒ ì»¬ëŸ¼
                    data_rows.append(row)
            
            # ì˜¤ëŠ˜(0), ì–´ì œ(1)
            if len(data_rows) >= 2:
                cols_today = data_rows[0].select('td')
                cols_yest = data_rows[1].select('td')
                
                if len(cols_today) > 0:
                    details['foreign_rate'] = cols_today[-1].get_text(strip=True)
                
                if len(cols_yest) > 0:
                     details['prev_foreign_rate'] = cols_yest[-1].get_text(strip=True)
                     
                     # ì–´ì œ ì¢…ê°€ (ì¸ë±ìŠ¤ 1)
                     prev_close_str = cols_yest[1].get_text(strip=True).replace(',', '')
                     if prev_close_str.isdigit():
                        details['prev_close'] = int(prev_close_str)
        
    except Exception as e:
        print(f"Error fetching foreign details for {code}: {e}")
        
    return details




def get_discussion_stats(code):
    """
    íŠ¹ì • ì¢…ëª© í† ë¡ ì‹¤ì˜ ê²Œì‹œê¸€ ì •ë³´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    - ë‹¹ì¼ 00:01 ì´í›„ ê²Œì‹œê¸€ ì •ë°€ ì¹´ìš´íŒ…
    - ìµœëŒ€ 800ê°œ ì œí•œ
    """
    
    # ê¸°ì¤€ ì‹œê°„ ì„¤ì • (ì‚¬ìš©ì ìš”ì²­ V7.4: ë‹¹ì¼ 08:00 ì´í›„)
    now = datetime.now()
    target_time = now.replace(hour=8, minute=0, second=0, microsecond=0)
    
    if now < target_time:
        pass 

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    collected_posts = []
    page = 1
    max_pages = 50 # v7.0 Tuning: Limit to ~1000 posts (User Request: 800)
    stop_collecting = False
    
    headers['Referer'] = f"https://finance.naver.com/item/board.naver?code={code}"

    while page <= max_pages and not stop_collecting:
        url = f"https://finance.naver.com/item/board.naver?code={code}&page={page}"
        
        try:
            if page > 1:
                time.sleep(0.5)

            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            table = soup.select_one('table.type2')
            if not table:
                break
                
            rows = table.select('tr')
            if not rows: 
                break
                
            # Max 800 check (User Request)
            if len(collected_posts) >= 800:
                stop_collecting = True
                break
                
            found_post_in_page = False
            
            for row in rows:
                cols = row.select('td')
                if len(cols) < 5:
                    continue
                
                try:
                    # ë‚ ì§œ í™•ì¸ "2024.05.21 14:30"
                    date_text = cols[0].get_text(strip=True)
                    
                    try:
                        post_date = datetime.strptime(date_text, "%Y.%m.%d %H:%M")
                    except ValueError:
                        continue
                        
                    found_post_in_page = True

                    # ê¸°ì¤€ ì‹œê°„ ì²´í¬
                    if post_date < target_time:
                        stop_collecting = True
                        break # ê³¼ê±° ê¸€
                    
                    # ìˆ˜ì§‘ ëŒ€ìƒ
                    title = ""
                    title_tag = row.select_one('a.title')
                    if not title_tag and len(cols) > 1:
                        title_tag = cols[1].select_one('a')
                    
                    if title_tag:
                         title = title_tag.get_text(strip=True)
                    
                    views = cols[3].get_text(strip=True)

                    collected_posts.append({
                        'title': title,
                        'date': date_text,
                        'views': views,
                        'likes': cols[4].get_text(strip=True) if len(cols) > 4 else '0',
                        'dislikes': cols[5].get_text(strip=True) if len(cols) > 5 else '0',
                        'link': title_tag['href'] if title_tag else ""
                    })
                    
                except Exception:
                    continue
            
            page += 1
            
        except Exception as e:
            print(f"Error fetching page {page} for {code}: {e}")
            break
            
    # Sort by Likes (Recomm) initially to pick candidates for Deep Dive
    collected_posts.sort(key=lambda x: int(x['likes']) if str(x['likes']).isdigit() else 0, reverse=True)

    return {
        'code': code,
        'recent_posts_count': len(collected_posts),
        'latest_posts': collected_posts, # Return ALL collected (will filter top 10 in main)
        'all_posts_titles': [p['title'] for p in collected_posts] 
    }

def fetch_post_body(link_suffix):
    """
    ê²Œì‹œê¸€ ë³¸ë¬¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (Deep Dive Analysis)
    link_suffix: /item/board_read.naver?code=...&nid=...
    """
    try:
        url = f"https://finance.naver.com{link_suffix}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://finance.naver.com/'
        }
        # Random sleep to be polite/safe
        time.sleep(0.3) 
        
        response = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Naver Finance Board Body Selector
        # specific ID or class might vary, usually 'div#body' or 'div.view_se'
        body_tag = soup.select_one('#body') or soup.select_one('.view_se') or soup.select_one('.scr01')
        
        if body_tag:
            return body_tag.get_text("\n", strip=True)
        return ""
    except Exception:
        return ""




import analyzer
from src import research_scraper
# from src import utils # Removed V7.0 (Legacy)

def load_env_manual(filepath=".env.local"):
    # Local .env support
    if os.path.exists(filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    key, val = line.strip().split('=', 1)
                    os.environ[key] = val

# --- Helper Functions (Added for V6.7 Fix) ---
def get_current_kst_time():
    """Returns current time in KST (UTC+9)."""
    # UTC time from GitHub Actions (or local system)
    now_utc = datetime.utcnow()
    now_kst = now_utc + timedelta(hours=9)
    return now_kst

def get_threshold_by_time(hour):
    """Returns the comment count threshold based on the hour (KST)."""
    # 10:00 run (covers 09:00 ~ 10:XX) -> Threshold 40 (Stricter)
    if 9 <= hour < 12:
        return 40
    # 13:00 run (covers 09:00 ~ 13:XX) -> Threshold 60
    elif 12 <= hour < 14:
        return 60
    # 15:00 run (covers 09:00 ~ 15:XX) -> Threshold 100
    elif 14 <= hour < 24:
        return 100
    return 10 # Default fallback

def get_yesterday_last_stocks():
    """
    reports.jsonì„ ë¶„ì„í•˜ì—¬ 'ì–´ì œ' ë‚ ì§œ ì¤‘ ê°€ì¥ ë§ˆì§€ë§‰ ìŠ¤ëƒ…ìƒ·(ë˜ëŠ” ë¦¬í¬íŠ¸)ì˜ ì¢…ëª© ì½”ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        reports_file = 'data/reports.json'
        if not os.path.exists(reports_file):
            return set()

        import json
        with open(reports_file, 'r', encoding='utf-8') as f:
            reports = json.load(f)
        
        # ì˜¤ëŠ˜ ë‚ ì§œ (KST ê¸°ì¤€)
        now_kst = get_current_kst_time()
        today_str = now_kst.strftime('%Y-%m-%d')
        
        # ì–´ì œ ë‚ ì§œ
        yesterday = now_kst - timedelta(days=1)
        yesterday_str = yesterday.strftime('%Y-%m-%d')
        
        # 1. reports.jsonì—ì„œ ì–´ì œ ë‚ ì§œì¸ ê²ƒë“¤ í•„í„°ë§
        # report['date'] í˜•ì‹: "2024-05-21 15:00"
        yesterday_reports = [
            r for r in reports 
            if r['date'].startswith(yesterday_str)
        ]
        
        if not yesterday_reports:
            # ì–´ì œ ë¦¬í¬íŠ¸ê°€ ì—†ë‹¤ë©´, ê·¸ ì „ì´ë¼ë„ ê°€ì ¸ì™€ì•¼ í•˜ë‚˜? 
            # ì‚¬ìš©ì ìš”ì²­: "ì–´ì œ ìˆ˜ì§‘í•œ ê°€ì¥ ë§ˆì§€ë§‰ ë°ì´í„°"
            # ì–´ì œê°€ íœ´ì¼ì¼ ìˆ˜ ìˆìŒ -> ì£¼ë§/íœ´ì¼ ì œì™¸ ë¡œì§ì´ ìˆë‹¤ë©´ ë°ì´í„°ê°€ ì—†ì„ ìˆ˜ ìˆìŒ.
            # ì¼ë‹¨ 'ì–´ì œ'ê°€ ìº˜ë¦°ë”ìƒ ì–´ì œì¸ì§€, ì§ì „ ì˜ì—…ì¼ì¸ì§€ ëª¨í˜¸í•˜ë‚˜ "ì–´ì œ"ë¡œ êµ¬í˜„.
            # ì§ì „ ì˜ì—…ì¼ë¡œ í•˜ë ¤ë©´ ë³µì¡í•´ì§. ì¼ë‹¨ ìº˜ë¦°ë” ì–´ì œë¡œ ì‹œë„.
            return set()
            
        # 2. ê°œì¤‘ ê°€ì¥ ë§ˆì§€ë§‰ ê²ƒ (reports.jsonì€ ìµœì‹ ìˆœ ì •ë ¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •, í˜¹ì€ timestamp í™•ì¸)
        # reports.jsonì€ insert(0, entry) í•˜ë¯€ë¡œ 0ë²ˆ ì¸ë±ìŠ¤ê°€ ìµœì‹ .
        # yesterday_reportsë„ ìˆœì„œ ìœ ì§€ëœë‹¤ë©´ 0ë²ˆì´ ê°€ì¥ ëŠ¦ì€ ì‹œê°„.
        last_report = yesterday_reports[0]
        filename = last_report['filename'] # trending_integrated_20240520_150000.xlsx
        
        # 3. íŒŒì¼ ë¡œë“œ (Excel)
        file_path = f"data/{filename}" # analyzer.save_data saves to current dir, usually root or relative?
        # scraper.py ì‹¤í–‰ ìœ„ì¹˜ ê¸°ì¤€. saved_files['excel']ì€ ë³´í†µ ìƒëŒ€ê²½ë¡œ(íŒŒì¼ëª…)ë§Œ ë¦¬í„´í•¨ (analyzer.py í™•ì¸ í•„ìš”)
        # analyzer.save_data: xlsx_filename = f"{base_name}.xlsx" -> í˜„ì¬ ë””ë ‰í† ë¦¬.
        
        if not os.path.exists(filename):
            # í˜¹ì‹œ data/ í´ë” ì•ˆì— ìˆì„ ìˆ˜ë„? (ì½”ë“œëŠ” í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥í•¨)
            # scraper.py: saved_files = analyzer.save_data(...) -> saved_files['excel'] returns filename.
            pass
            
        import pandas as pd
        if filename.endswith('.xlsx'):
            df = pd.read_excel(filename)
        elif filename.endswith('.csv'):
            df = pd.read_csv(filename)
        else:
            return set()
            
        # 'code' or 'ì¢…ëª©ì½”ë“œ' ì»¬ëŸ¼ ì¶”ì¶œ
        # analyzer.py result_df_kr ì»¬ëŸ¼: 'ì¢…ëª©ì½”ë“œ'
        if 'ì¢…ëª©ì½”ë“œ' in df.columns:
            return set(df['ì¢…ëª©ì½”ë“œ'].astype(str).str.zfill(6).tolist())
        
        return set()
        
    except Exception as e:
        print(f"[Warning] Failed to get yesterday's stocks: {e}")
        return set()

if __name__ == "__main__":
    # 0. Load Environment Variables
    load_env_manual()
    
    # 1. Initialize Time & Threshold (CRITICAL FIX V6.7)
    now_kst = get_current_kst_time()
    current_hour = now_kst.hour
    threshold = get_threshold_by_time(current_hour)
    
    now = now_kst # Sync variable name for later use
    
    print(f"[System] Time (KST): {now_kst.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # --- Market Holiday Check (V6.8) ---
    import holidays
    kr_holidays = holidays.KR()
    
    is_weekend = now_kst.weekday() >= 5 # 5=Sat, 6=Sun
    is_holiday = now_kst.strftime('%Y-%m-%d') in kr_holidays
    
    if is_weekend or is_holiday:
        reason = "Weekend" if is_weekend else f"Holiday ({kr_holidays.get(now_kst.strftime('%Y-%m-%d'))})"
        print(f"[System] Market Closed Today ({reason}). Skipping execution.")
        sys.exit(0) # Exit cleanly, no Telegram sent.
        
    print(f"[System] Threshold determined: {threshold} posts (based on hour {current_hour})")

    # --- 0. Initialize Telegram Manager (V7.0) ---
    try:
        from src.telegram_manager import TelegramManager
        tg_manager = TelegramManager()
        # Dashboard Link moved to end
    except Exception as e:
        print(f"[System] Failed to initialize TelegramManager: {e}")
        tg_manager = None
    # 2. Research Briefing (Enabled)
    print("\n[Research] Updating Market Briefing & PDF Analysis...")
    try:
        from src import research_scraper # Ensure import
        research_scraper.main()
        print("[Research] Completed.")
        
        # Send Research Telegram
        try:
            import json
            # Correct path matches research_scraper.py output (data/latest_research.json)
            with open('data/latest_research.json', 'r', encoding='utf-8') as f:
                r_data = json.load(f)
            
            invest_summary = r_data.get('invest', {}).get('summary', 'ìš”ì•½ ì—†ìŒ')
            items_count = r_data.get('invest', {}).get('today_count', 0)
            
            r_msg = f"ğŸ“‘ <b>[ë¦¬í¬íŠ¸ ë¸Œë¦¬í•‘] ì´ {items_count}ê±´</b>\n\n"
            r_msg += f"ğŸ’¡ ì‹œì¥ ìš”ì•½: {invest_summary[:300]}...\n\n"
            r_msg += f"ğŸ‘‰ ìì„¸íˆ ë³´ê¸°: {os.environ.get('DASHBOARD_URL', '')}"
            
            # tg_manager.send_message(r_msg) # User requested to disable Research Briefing (V7.1)
            print("[Research] Telegram Sent (Disabled by User Request).")
            
        except Exception as tg_e:
            print(f"[Research] Telegram Error: {tg_e}")
            
    except Exception as e:
        print(f"[Research] Error: {e}")

    markets = ['KOSPI', 'KOSDAQ']
    # ... (rest of code) ...
    
    all_data = [] # í†µí•© ë°ì´í„° ì €ì¥ìš©

    today_consecutive_check_done = False
    yesterday_codes = set()
    
    # [Consecutive Check V7.4]
    try:
        yesterday_codes = get_yesterday_last_stocks()
        print(f"[System] Loaded {len(yesterday_codes)} stocks from yesterday for consecutive check.")
    except Exception as e:
        print(f"[System] Consecutive check setup failed: {e}")

    for market in markets:
        if market == 'KOSDAQ':
             print("Wait 5 seconds before KOSDAQ...", flush=True)
             time.sleep(5)

        print(f"\n[{market}] Starting collection...")
        # Get MORE stocks to ensure we find enough active ones (Top 50)
        trending_stocks = get_top_trending_stocks(market)
        # Limit to top 50 (Apply function limit)
        # Assuming get_top_trending_stocks returns whatever it finds on page (usually 100 if not sliced)
        
        # In this edited version, we'll slice larger
        source_count = len(trending_stocks)
        print(f"Found {source_count} stocks in {market} Top list.")
        
        count_collected = 0
        
        for i, stock in enumerate(trending_stocks):
            # Performance safety / Limit (User Request V7.0: 20 stocks)
            if i >= 20: break 
            
            # 1. ìƒì„¸ ì •ë³´ (ì „ì¼ì¢…ê°€, ì™¸êµ­ì¸)
            details = get_stock_details(stock['code'])
            stock.update(details)
            
            # 2. í† ë¡ ë°© ì •ë³´ (ì‹œê°„ ê¸°ì¤€ ì¹´ìš´íŒ…)
            stats = get_discussion_stats(stock['code'])
            recent_count = stats.get('recent_posts_count', 0)
            
            # FILTER HERE
            # FILTER HERE
            if recent_count >= threshold:
                stock['recent_posts_count'] = recent_count
                
                # [Deep Dive V7.5] Analyze Top 10 Liked Posts
                raw_latest = stats.get('latest_posts', [])
                # Take Top 10 (Already sorted by likes in get_discussion_stats? No, we need to ensure int sort there or here)
                # Ensure sort by likes descending
                raw_latest.sort(key=lambda x: int(x['likes']) if str(x['likes']).isdigit() else 0, reverse=True)
                candidates = raw_latest[:10]
                
                print(f"   [Deep Dive] Fetching body for {len(candidates)} posts...")
                for post in candidates:
                    if post.get('link'):
                        post['body'] = fetch_post_body(post['link'])
                    else:
                        post['body'] = ""
                
                stock['latest_posts'] = candidates # Assign enriched posts
                stock['all_posts_titles'] = stats.get('all_posts_titles', []) 
                
                # Consecutive Flag
                if stock['code'] in yesterday_codes:
                    stock['is_consecutive'] = True
                    # Legacy 'summary' field update for frontend display if needed
                    # stock['posts_summary'] = "[ì—°ì†] " + stock.get('posts_summary', '') 
                else:
                    stock['is_consecutive'] = False

                all_data.append(stock)
                count_collected += 1
                print(f" [KEEP] {stock['name']}: {recent_count} posts (Threshold {threshold})")
            else:
                # print(f" [SKIP] {stock['name']}: {recent_count} posts")
                pass

        print(f"Collected {count_collected} items from {market} meeting criteria.")

    # --- 5. Telegram Notification (Refactored V7.0 - Zero Base) ---
    try:
        from src.telegram_manager import TelegramManager
        try:
            tg_manager = TelegramManager()
            # DEBUG: Check credentials
            print(f"[DEBUG] Telegram Token Loaded: {bool(tg_manager.token)}")
            print(f"[DEBUG] Telegram Chat ID Loaded: {bool(tg_manager.chat_id)}")
        except Exception as e:
            print(f"[WARNING] Failed to initialize TelegramManager: {e}")
            tg_manager = None

        # Prepare Data for Saving (Always, even if empty)
        import json
        os.makedirs('data', exist_ok=True)
        
        if all_data:
            print(f"\nAnalyzing total {len(all_data)} items...")
            result_df_kr, result_df_en = analyzer.analyze_discussion_trend(all_data)
            json_records = result_df_en.to_dict('records')
            
            # Save CSV & Excel (History)
            filename_prefix = f"trending_integrated"
            saved_files = analyzer.save_data(result_df_kr, filename_prefix=filename_prefix)
            
            # --- Update Reports Index (reports.json) ---
            if 'excel' in saved_files:
                report_entry = {
                    "date": now_kst.strftime('%Y-%m-%d %H:%M'),
                    "filename": os.path.basename(saved_files['excel']),
                    "count": len(all_data),
                    "timestamp": datetime.now().timestamp()
                }
                
                reports_file = 'data/reports.json'
                current_reports = []
                if os.path.exists(reports_file):
                    try:
                        with open(reports_file, 'r', encoding='utf-8') as f:
                            current_reports = json.load(f)
                    except:
                        pass
                
                # Prepend new report (Latest first)
                current_reports.insert(0, report_entry)
                # Keep last 50
                current_reports = current_reports[:50]
                
                with open(reports_file, 'w', encoding='utf-8') as f:
                    json.dump(current_reports, f, ensure_ascii=False, indent=2)
                print(f"[System] Updated reports index: {reports_file}")
                
        else:
            print(f"\n[System] No data collected (all below threshold {threshold}). Saving empty records.")
            json_records = []
            result_df_kr = None

        # Save JSON for Frontend (latest_stocks.json) - ALWAYS
        with open('data/latest_stocks.json', 'w', encoding='utf-8') as f:
            json.dump(json_records, f, ensure_ascii=False, indent=2)
        print(f"Data saved to data/latest_stocks.json (Count: {len(json_records)})")

        # [User Request V7.3] Save Time-Specific Snapshot - ALWAYS
        snapshot_name = None
        if 9 <= current_hour <= 10: snapshot_name = "stocks_1000.json"
        elif 12 <= current_hour <= 13: snapshot_name = "stocks_1300.json"
        elif 14 <= current_hour <= 23: snapshot_name = "stocks_1500.json" # Covers 14:00 ~ Midnight (Closing Data)
        
        if snapshot_name:
            with open(f'data/{snapshot_name}', 'w', encoding='utf-8') as f:
                json.dump(json_records, f, ensure_ascii=False, indent=2)
            print(f"Snapshot saved: data/{snapshot_name} (Count: {len(json_records)})")

        # Telegram Notifications
        if all_data:
            if tg_manager:
                try:
                    # Filter Lists
                    records = result_df_kr.to_dict('records')
                    kospi_items = [r for r in records if r.get('ì‹œì¥êµ¬ë¶„') == 'KOSPI']
                    kosdaq_items = [r for r in records if r.get('ì‹œì¥êµ¬ë¶„') == 'KOSDAQ']
                    
                    if kospi_items:
                        tg_manager.send_market_report('KOSPI', kospi_items)
                        time.sleep(1)
                        
                    if kosdaq_items:
                        tg_manager.send_market_report('KOSDAQ', kosdaq_items)
                        time.sleep(1)

                    # 2. Dashboard Link
                    print(f"[System] Sending Dashboard Link last... (v7.0)")
                    tg_manager.send_dashboard_link()
                except Exception as send_err:
                    print(f"[ERROR] details sending Telegram: {send_err}")
            else:
                 print("[System] TelegramManager not available. Skipping notifications.")
        else:
            print("No data collected meeting the threshold.")
            if tg_manager:
                print(f"[System] Sending No Data Alert (Threshold: {threshold})")
                try:
                    tg_manager.send_no_data_alert(threshold)
                except Exception as e:
                    print(f"[ERROR] Failed to send No Data Alert: {e}")

    except Exception as e:
        print(f"Failed in notification/saving section: {e}")

    finally:
        # Save Status JSON for Frontend (ALWAYS RUN)
        try:
            import json
            status_data = {
                "last_updated": now_kst.strftime('%Y-%m-%d %H:%M:%S'),
                "message": "Data updated successfully" if all_data else "No data collected",
                "count": len(all_data) if 'all_data' in locals() else 0
            }
            with open('data/status.json', 'w', encoding='utf-8') as f:
                json.dump(status_data, f, ensure_ascii=False, indent=2)
            print(f"[System] status.json updated at {status_data['last_updated']}")
        except Exception as status_e:
            print(f"[ERROR] Failed to save status.json: {status_e}")









